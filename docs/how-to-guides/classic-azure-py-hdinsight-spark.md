---
title: "Spark on Azure HDInsight | Python"
h1: "Spark on Azure HDInsight"
linktitle: "Spark on Azure HDInsight"
meta_desc: "Spark on Azure HDInsight How-to Guide using Python"
no_edit_this_page: true
cloud: classic-azure
language: py
layout: package
---

<!-- WARNING: this page was generated by a tool. Do not edit it by hand. -->
<!-- To change it, please see https://github.com/pulumi/registry/tree/master/tools/mktutorial. -->

<p class="mb-4 inline-flex items-center">
    <a class="rounded-md font-display text-lg text-white bg-white border-2 border-blue-600 px-3 mr-2 whitespace-no-wrap hover:text-white" style="height: 45px; line-height: 41px;" href="https://github.com/pulumi/examples/tree/master/classic-azure-py-hdinsight-spark" target="_blank">
        <span class="flex items-center">
            <i class="fab fa-github pr-1.5"></i>
            <span>View Code</span>
        </span>
    </a>
    <a href="https://app.pulumi.com/new?template=https://github.com/pulumi/examples/blob/master/classic-azure-py-hdinsight-spark/README.md" target="_blank">
        <img src="https://get.pulumi.com/new/button.svg" alt="Deploy this example with Pulumi">
    </a>
</p>


An example Pulumi component that deploys a Spark cluster on Azure HDInsight.

## Running the App

1. Create a new stack:

    ```bash
    $ pulumi stack init dev
    ```

1. Login to Azure CLI (you will be prompted to do this during deployment if you forget this step):

    ```bash
    $ az login
    ```

1. Specify the Azure location to use:

    ```bash
    $ pulumi config set azure:location WestUS
    ```

1. Define Spark username and password (make it complex enough to satisfy Azure policy):

    ```bash
    $ pulumi config set username <value>
    $ pulumi config set --secret password <value>
    ```

1. Run `pulumi up` to preview and deploy changes:

    ``` bash
    $ pulumi up
    Previewing changes:
    ...

    Performing changes:
    ...
    info: 5 changes performed:
        + 5 resources created
    Update duration: 15m6s
    ```

1. Check the deployed Spark endpoint:

    ```bash
    $ pulumi stack output endpoint
    https://myspark1234abcd.azurehdinsight.net/

    # For instance, Jupyter notebooks are available at https://myspark1234abcd.azurehdinsight.net/jupyter/
    # Follow https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-load-data-run-query to test it out
    ```

