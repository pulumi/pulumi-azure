// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * Manages a Synapse Spark Pool.
 *
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as azure from "@pulumi/azure";
 *
 * const example = new azure.core.ResourceGroup("example", {
 *     name: "example-resources",
 *     location: "West Europe",
 * });
 * const exampleAccount = new azure.storage.Account("example", {
 *     name: "examplestorageacc",
 *     resourceGroupName: example.name,
 *     location: example.location,
 *     accountTier: "Standard",
 *     accountReplicationType: "LRS",
 *     accountKind: "StorageV2",
 *     isHnsEnabled: true,
 * });
 * const exampleDataLakeGen2Filesystem = new azure.storage.DataLakeGen2Filesystem("example", {
 *     name: "example",
 *     storageAccountId: exampleAccount.id,
 * });
 * const exampleWorkspace = new azure.synapse.Workspace("example", {
 *     name: "example",
 *     resourceGroupName: example.name,
 *     location: example.location,
 *     storageDataLakeGen2FilesystemId: exampleDataLakeGen2Filesystem.id,
 *     sqlAdministratorLogin: "sqladminuser",
 *     sqlAdministratorLoginPassword: "H@Sh1CoR3!",
 *     identity: {
 *         type: "SystemAssigned",
 *     },
 * });
 * const exampleSparkPool = new azure.synapse.SparkPool("example", {
 *     name: "example",
 *     synapseWorkspaceId: exampleWorkspace.id,
 *     nodeSizeFamily: "MemoryOptimized",
 *     nodeSize: "Small",
 *     cacheSize: 100,
 *     autoScale: {
 *         maxNodeCount: 50,
 *         minNodeCount: 3,
 *     },
 *     autoPause: {
 *         delayInMinutes: 15,
 *     },
 *     libraryRequirement: {
 *         content: `appnope==0.1.0
 * beautifulsoup4==4.6.3
 * `,
 *         filename: "requirements.txt",
 *     },
 *     sparkConfig: {
 *         content: "spark.shuffle.spill                true\n",
 *         filename: "config.txt",
 *     },
 *     tags: {
 *         ENV: "Production",
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * Synapse Spark Pool can be imported using the `resource id`, e.g.
 *
 * ```sh
 * $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
 * ```
 */
export class SparkPool extends pulumi.CustomResource {
    /**
     * Get an existing SparkPool resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: SparkPoolState, opts?: pulumi.CustomResourceOptions): SparkPool {
        return new SparkPool(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'azure:synapse/sparkPool:SparkPool';

    /**
     * Returns true if the given object is an instance of SparkPool.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is SparkPool {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === SparkPool.__pulumiType;
    }

    /**
     * An `autoPause` block as defined below.
     */
    public readonly autoPause!: pulumi.Output<outputs.synapse.SparkPoolAutoPause | undefined>;
    /**
     * An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    public readonly autoScale!: pulumi.Output<outputs.synapse.SparkPoolAutoScale | undefined>;
    /**
     * The cache size in the Spark Pool.
     */
    public readonly cacheSize!: pulumi.Output<number | undefined>;
    /**
     * Indicates whether compute isolation is enabled or not. Defaults to `false`.
     */
    public readonly computeIsolationEnabled!: pulumi.Output<boolean | undefined>;
    public readonly dynamicExecutorAllocationEnabled!: pulumi.Output<boolean | undefined>;
    public readonly libraryRequirement!: pulumi.Output<outputs.synapse.SparkPoolLibraryRequirement | undefined>;
    public readonly maxExecutors!: pulumi.Output<number | undefined>;
    public readonly minExecutors!: pulumi.Output<number | undefined>;
    /**
     * The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    public readonly nodeCount!: pulumi.Output<number | undefined>;
    /**
     * The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     */
    public readonly nodeSize!: pulumi.Output<string>;
    /**
     * The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     */
    public readonly nodeSizeFamily!: pulumi.Output<string>;
    public readonly sessionLevelPackagesEnabled!: pulumi.Output<boolean | undefined>;
    public readonly sparkConfig!: pulumi.Output<outputs.synapse.SparkPoolSparkConfig | undefined>;
    public readonly sparkEventsFolder!: pulumi.Output<string | undefined>;
    public readonly sparkLogFolder!: pulumi.Output<string | undefined>;
    public readonly sparkVersion!: pulumi.Output<string | undefined>;
    /**
     * The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     */
    public readonly synapseWorkspaceId!: pulumi.Output<string>;
    public readonly tags!: pulumi.Output<{[key: string]: string} | undefined>;

    /**
     * Create a SparkPool resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: SparkPoolArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: SparkPoolArgs | SparkPoolState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as SparkPoolState | undefined;
            resourceInputs["autoPause"] = state ? state.autoPause : undefined;
            resourceInputs["autoScale"] = state ? state.autoScale : undefined;
            resourceInputs["cacheSize"] = state ? state.cacheSize : undefined;
            resourceInputs["computeIsolationEnabled"] = state ? state.computeIsolationEnabled : undefined;
            resourceInputs["dynamicExecutorAllocationEnabled"] = state ? state.dynamicExecutorAllocationEnabled : undefined;
            resourceInputs["libraryRequirement"] = state ? state.libraryRequirement : undefined;
            resourceInputs["maxExecutors"] = state ? state.maxExecutors : undefined;
            resourceInputs["minExecutors"] = state ? state.minExecutors : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["nodeCount"] = state ? state.nodeCount : undefined;
            resourceInputs["nodeSize"] = state ? state.nodeSize : undefined;
            resourceInputs["nodeSizeFamily"] = state ? state.nodeSizeFamily : undefined;
            resourceInputs["sessionLevelPackagesEnabled"] = state ? state.sessionLevelPackagesEnabled : undefined;
            resourceInputs["sparkConfig"] = state ? state.sparkConfig : undefined;
            resourceInputs["sparkEventsFolder"] = state ? state.sparkEventsFolder : undefined;
            resourceInputs["sparkLogFolder"] = state ? state.sparkLogFolder : undefined;
            resourceInputs["sparkVersion"] = state ? state.sparkVersion : undefined;
            resourceInputs["synapseWorkspaceId"] = state ? state.synapseWorkspaceId : undefined;
            resourceInputs["tags"] = state ? state.tags : undefined;
        } else {
            const args = argsOrState as SparkPoolArgs | undefined;
            if ((!args || args.nodeSize === undefined) && !opts.urn) {
                throw new Error("Missing required property 'nodeSize'");
            }
            if ((!args || args.nodeSizeFamily === undefined) && !opts.urn) {
                throw new Error("Missing required property 'nodeSizeFamily'");
            }
            if ((!args || args.synapseWorkspaceId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'synapseWorkspaceId'");
            }
            resourceInputs["autoPause"] = args ? args.autoPause : undefined;
            resourceInputs["autoScale"] = args ? args.autoScale : undefined;
            resourceInputs["cacheSize"] = args ? args.cacheSize : undefined;
            resourceInputs["computeIsolationEnabled"] = args ? args.computeIsolationEnabled : undefined;
            resourceInputs["dynamicExecutorAllocationEnabled"] = args ? args.dynamicExecutorAllocationEnabled : undefined;
            resourceInputs["libraryRequirement"] = args ? args.libraryRequirement : undefined;
            resourceInputs["maxExecutors"] = args ? args.maxExecutors : undefined;
            resourceInputs["minExecutors"] = args ? args.minExecutors : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["nodeCount"] = args ? args.nodeCount : undefined;
            resourceInputs["nodeSize"] = args ? args.nodeSize : undefined;
            resourceInputs["nodeSizeFamily"] = args ? args.nodeSizeFamily : undefined;
            resourceInputs["sessionLevelPackagesEnabled"] = args ? args.sessionLevelPackagesEnabled : undefined;
            resourceInputs["sparkConfig"] = args ? args.sparkConfig : undefined;
            resourceInputs["sparkEventsFolder"] = args ? args.sparkEventsFolder : undefined;
            resourceInputs["sparkLogFolder"] = args ? args.sparkLogFolder : undefined;
            resourceInputs["sparkVersion"] = args ? args.sparkVersion : undefined;
            resourceInputs["synapseWorkspaceId"] = args ? args.synapseWorkspaceId : undefined;
            resourceInputs["tags"] = args ? args.tags : undefined;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(SparkPool.__pulumiType, name, resourceInputs, opts, false /*remote*/);
    }
}

/**
 * Input properties used for looking up and filtering SparkPool resources.
 */
export interface SparkPoolState {
    /**
     * An `autoPause` block as defined below.
     */
    autoPause?: pulumi.Input<inputs.synapse.SparkPoolAutoPause>;
    /**
     * An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    autoScale?: pulumi.Input<inputs.synapse.SparkPoolAutoScale>;
    /**
     * The cache size in the Spark Pool.
     */
    cacheSize?: pulumi.Input<number>;
    /**
     * Indicates whether compute isolation is enabled or not. Defaults to `false`.
     */
    computeIsolationEnabled?: pulumi.Input<boolean>;
    dynamicExecutorAllocationEnabled?: pulumi.Input<boolean>;
    libraryRequirement?: pulumi.Input<inputs.synapse.SparkPoolLibraryRequirement>;
    maxExecutors?: pulumi.Input<number>;
    minExecutors?: pulumi.Input<number>;
    /**
     * The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     */
    name?: pulumi.Input<string>;
    /**
     * The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     */
    nodeSize?: pulumi.Input<string>;
    /**
     * The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     */
    nodeSizeFamily?: pulumi.Input<string>;
    sessionLevelPackagesEnabled?: pulumi.Input<boolean>;
    sparkConfig?: pulumi.Input<inputs.synapse.SparkPoolSparkConfig>;
    sparkEventsFolder?: pulumi.Input<string>;
    sparkLogFolder?: pulumi.Input<string>;
    sparkVersion?: pulumi.Input<string>;
    /**
     * The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     */
    synapseWorkspaceId?: pulumi.Input<string>;
    tags?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
}

/**
 * The set of arguments for constructing a SparkPool resource.
 */
export interface SparkPoolArgs {
    /**
     * An `autoPause` block as defined below.
     */
    autoPause?: pulumi.Input<inputs.synapse.SparkPoolAutoPause>;
    /**
     * An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    autoScale?: pulumi.Input<inputs.synapse.SparkPoolAutoScale>;
    /**
     * The cache size in the Spark Pool.
     */
    cacheSize?: pulumi.Input<number>;
    /**
     * Indicates whether compute isolation is enabled or not. Defaults to `false`.
     */
    computeIsolationEnabled?: pulumi.Input<boolean>;
    dynamicExecutorAllocationEnabled?: pulumi.Input<boolean>;
    libraryRequirement?: pulumi.Input<inputs.synapse.SparkPoolLibraryRequirement>;
    maxExecutors?: pulumi.Input<number>;
    minExecutors?: pulumi.Input<number>;
    /**
     * The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     */
    name?: pulumi.Input<string>;
    /**
     * The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     */
    nodeSize: pulumi.Input<string>;
    /**
     * The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     */
    nodeSizeFamily: pulumi.Input<string>;
    sessionLevelPackagesEnabled?: pulumi.Input<boolean>;
    sparkConfig?: pulumi.Input<inputs.synapse.SparkPoolSparkConfig>;
    sparkEventsFolder?: pulumi.Input<string>;
    sparkLogFolder?: pulumi.Input<string>;
    sparkVersion?: pulumi.Input<string>;
    /**
     * The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     */
    synapseWorkspaceId: pulumi.Input<string>;
    tags?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
}
