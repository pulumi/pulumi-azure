// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.azure.synapse;

import com.pulumi.azure.Utilities;
import com.pulumi.azure.synapse.SparkPoolArgs;
import com.pulumi.azure.synapse.inputs.SparkPoolState;
import com.pulumi.azure.synapse.outputs.SparkPoolAutoPause;
import com.pulumi.azure.synapse.outputs.SparkPoolAutoScale;
import com.pulumi.azure.synapse.outputs.SparkPoolLibraryRequirement;
import com.pulumi.azure.synapse.outputs.SparkPoolSparkConfig;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Manages a Synapse Spark Pool.
 * 
 * ## Example Usage
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.azure.core.ResourceGroup;
 * import com.pulumi.azure.core.ResourceGroupArgs;
 * import com.pulumi.azure.storage.Account;
 * import com.pulumi.azure.storage.AccountArgs;
 * import com.pulumi.azure.storage.DataLakeGen2Filesystem;
 * import com.pulumi.azure.storage.DataLakeGen2FilesystemArgs;
 * import com.pulumi.azure.synapse.Workspace;
 * import com.pulumi.azure.synapse.WorkspaceArgs;
 * import com.pulumi.azure.synapse.inputs.WorkspaceIdentityArgs;
 * import com.pulumi.azure.synapse.SparkPool;
 * import com.pulumi.azure.synapse.SparkPoolArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolAutoScaleArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolAutoPauseArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolLibraryRequirementArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolSparkConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var example = new ResourceGroup("example", ResourceGroupArgs.builder()        
 *             .name("example-resources")
 *             .location("West Europe")
 *             .build());
 * 
 *         var exampleAccount = new Account("exampleAccount", AccountArgs.builder()        
 *             .name("examplestorageacc")
 *             .resourceGroupName(example.name())
 *             .location(example.location())
 *             .accountTier("Standard")
 *             .accountReplicationType("LRS")
 *             .accountKind("StorageV2")
 *             .isHnsEnabled("true")
 *             .build());
 * 
 *         var exampleDataLakeGen2Filesystem = new DataLakeGen2Filesystem("exampleDataLakeGen2Filesystem", DataLakeGen2FilesystemArgs.builder()        
 *             .name("example")
 *             .storageAccountId(exampleAccount.id())
 *             .build());
 * 
 *         var exampleWorkspace = new Workspace("exampleWorkspace", WorkspaceArgs.builder()        
 *             .name("example")
 *             .resourceGroupName(example.name())
 *             .location(example.location())
 *             .storageDataLakeGen2FilesystemId(exampleDataLakeGen2Filesystem.id())
 *             .sqlAdministratorLogin("sqladminuser")
 *             .sqlAdministratorLoginPassword("H{@literal @}Sh1CoR3!")
 *             .identity(WorkspaceIdentityArgs.builder()
 *                 .type("SystemAssigned")
 *                 .build())
 *             .build());
 * 
 *         var exampleSparkPool = new SparkPool("exampleSparkPool", SparkPoolArgs.builder()        
 *             .name("example")
 *             .synapseWorkspaceId(exampleWorkspace.id())
 *             .nodeSizeFamily("MemoryOptimized")
 *             .nodeSize("Small")
 *             .cacheSize(100)
 *             .autoScale(SparkPoolAutoScaleArgs.builder()
 *                 .maxNodeCount(50)
 *                 .minNodeCount(3)
 *                 .build())
 *             .autoPause(SparkPoolAutoPauseArgs.builder()
 *                 .delayInMinutes(15)
 *                 .build())
 *             .libraryRequirement(SparkPoolLibraryRequirementArgs.builder()
 *                 .content("""
 * appnope==0.1.0
 * beautifulsoup4==4.6.3
 *                 """)
 *                 .filename("requirements.txt")
 *                 .build())
 *             .sparkConfig(SparkPoolSparkConfigArgs.builder()
 *                 .content("""
 * spark.shuffle.spill                true
 *                 """)
 *                 .filename("config.txt")
 *                 .build())
 *             .tags(Map.of("ENV", "Production"))
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Import
 * 
 * Synapse Spark Pool can be imported using the `resource id`, e.g.
 * 
 * ```sh
 * $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
 * ```
 * 
 */
@ResourceType(type="azure:synapse/sparkPool:SparkPool")
public class SparkPool extends com.pulumi.resources.CustomResource {
    /**
     * An `auto_pause` block as defined below.
     * 
     */
    @Export(name="autoPause", refs={SparkPoolAutoPause.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolAutoPause> autoPause;

    /**
     * @return An `auto_pause` block as defined below.
     * 
     */
    public Output<Optional<SparkPoolAutoPause>> autoPause() {
        return Codegen.optional(this.autoPause);
    }
    /**
     * An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    @Export(name="autoScale", refs={SparkPoolAutoScale.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolAutoScale> autoScale;

    /**
     * @return An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    public Output<Optional<SparkPoolAutoScale>> autoScale() {
        return Codegen.optional(this.autoScale);
    }
    /**
     * The cache size in the Spark Pool.
     * 
     */
    @Export(name="cacheSize", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> cacheSize;

    /**
     * @return The cache size in the Spark Pool.
     * 
     */
    public Output<Optional<Integer>> cacheSize() {
        return Codegen.optional(this.cacheSize);
    }
    /**
     * Indicates whether compute isolation is enabled or not. Defaults to `false`.
     * 
     */
    @Export(name="computeIsolationEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> computeIsolationEnabled;

    /**
     * @return Indicates whether compute isolation is enabled or not. Defaults to `false`.
     * 
     */
    public Output<Optional<Boolean>> computeIsolationEnabled() {
        return Codegen.optional(this.computeIsolationEnabled);
    }
    @Export(name="dynamicExecutorAllocationEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> dynamicExecutorAllocationEnabled;

    public Output<Optional<Boolean>> dynamicExecutorAllocationEnabled() {
        return Codegen.optional(this.dynamicExecutorAllocationEnabled);
    }
    @Export(name="libraryRequirement", refs={SparkPoolLibraryRequirement.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolLibraryRequirement> libraryRequirement;

    public Output<Optional<SparkPoolLibraryRequirement>> libraryRequirement() {
        return Codegen.optional(this.libraryRequirement);
    }
    @Export(name="maxExecutors", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> maxExecutors;

    public Output<Optional<Integer>> maxExecutors() {
        return Codegen.optional(this.maxExecutors);
    }
    @Export(name="minExecutors", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> minExecutors;

    public Output<Optional<Integer>> minExecutors() {
        return Codegen.optional(this.minExecutors);
    }
    /**
     * The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    /**
     * The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    @Export(name="nodeCount", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> nodeCount;

    /**
     * @return The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    public Output<Optional<Integer>> nodeCount() {
        return Codegen.optional(this.nodeCount);
    }
    /**
     * The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     * 
     */
    @Export(name="nodeSize", refs={String.class}, tree="[0]")
    private Output<String> nodeSize;

    /**
     * @return The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     * 
     */
    public Output<String> nodeSize() {
        return this.nodeSize;
    }
    /**
     * The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     * 
     */
    @Export(name="nodeSizeFamily", refs={String.class}, tree="[0]")
    private Output<String> nodeSizeFamily;

    /**
     * @return The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     * 
     */
    public Output<String> nodeSizeFamily() {
        return this.nodeSizeFamily;
    }
    @Export(name="sessionLevelPackagesEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> sessionLevelPackagesEnabled;

    public Output<Optional<Boolean>> sessionLevelPackagesEnabled() {
        return Codegen.optional(this.sessionLevelPackagesEnabled);
    }
    @Export(name="sparkConfig", refs={SparkPoolSparkConfig.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolSparkConfig> sparkConfig;

    public Output<Optional<SparkPoolSparkConfig>> sparkConfig() {
        return Codegen.optional(this.sparkConfig);
    }
    @Export(name="sparkEventsFolder", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkEventsFolder;

    public Output<Optional<String>> sparkEventsFolder() {
        return Codegen.optional(this.sparkEventsFolder);
    }
    @Export(name="sparkLogFolder", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkLogFolder;

    public Output<Optional<String>> sparkLogFolder() {
        return Codegen.optional(this.sparkLogFolder);
    }
    @Export(name="sparkVersion", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkVersion;

    public Output<Optional<String>> sparkVersion() {
        return Codegen.optional(this.sparkVersion);
    }
    /**
     * The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    @Export(name="synapseWorkspaceId", refs={String.class}, tree="[0]")
    private Output<String> synapseWorkspaceId;

    /**
     * @return The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    public Output<String> synapseWorkspaceId() {
        return this.synapseWorkspaceId;
    }
    @Export(name="tags", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> tags;

    public Output<Optional<Map<String,String>>> tags() {
        return Codegen.optional(this.tags);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public SparkPool(String name) {
        this(name, SparkPoolArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public SparkPool(String name, SparkPoolArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public SparkPool(String name, SparkPoolArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("azure:synapse/sparkPool:SparkPool", name, args == null ? SparkPoolArgs.Empty : args, makeResourceOptions(options, Codegen.empty()));
    }

    private SparkPool(String name, Output<String> id, @Nullable SparkPoolState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("azure:synapse/sparkPool:SparkPool", name, state, makeResourceOptions(options, id));
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static SparkPool get(String name, Output<String> id, @Nullable SparkPoolState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new SparkPool(name, id, state, options);
    }
}
