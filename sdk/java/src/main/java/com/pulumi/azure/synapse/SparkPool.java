// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.azure.synapse;

import com.pulumi.azure.Utilities;
import com.pulumi.azure.synapse.SparkPoolArgs;
import com.pulumi.azure.synapse.inputs.SparkPoolState;
import com.pulumi.azure.synapse.outputs.SparkPoolAutoPause;
import com.pulumi.azure.synapse.outputs.SparkPoolAutoScale;
import com.pulumi.azure.synapse.outputs.SparkPoolLibraryRequirement;
import com.pulumi.azure.synapse.outputs.SparkPoolSparkConfig;
import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Manages a Synapse Spark Pool.
 * 
 * ## Example Usage
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.azure.core.ResourceGroup;
 * import com.pulumi.azure.core.ResourceGroupArgs;
 * import com.pulumi.azure.storage.Account;
 * import com.pulumi.azure.storage.AccountArgs;
 * import com.pulumi.azure.storage.DataLakeGen2Filesystem;
 * import com.pulumi.azure.storage.DataLakeGen2FilesystemArgs;
 * import com.pulumi.azure.synapse.Workspace;
 * import com.pulumi.azure.synapse.WorkspaceArgs;
 * import com.pulumi.azure.synapse.inputs.WorkspaceIdentityArgs;
 * import com.pulumi.azure.synapse.SparkPool;
 * import com.pulumi.azure.synapse.SparkPoolArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolAutoScaleArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolAutoPauseArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolLibraryRequirementArgs;
 * import com.pulumi.azure.synapse.inputs.SparkPoolSparkConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var example = new ResourceGroup(&#34;example&#34;, ResourceGroupArgs.builder()        
 *             .name(&#34;example-resources&#34;)
 *             .location(&#34;West Europe&#34;)
 *             .build());
 * 
 *         var exampleAccount = new Account(&#34;exampleAccount&#34;, AccountArgs.builder()        
 *             .name(&#34;examplestorageacc&#34;)
 *             .resourceGroupName(example.name())
 *             .location(example.location())
 *             .accountTier(&#34;Standard&#34;)
 *             .accountReplicationType(&#34;LRS&#34;)
 *             .accountKind(&#34;StorageV2&#34;)
 *             .isHnsEnabled(&#34;true&#34;)
 *             .build());
 * 
 *         var exampleDataLakeGen2Filesystem = new DataLakeGen2Filesystem(&#34;exampleDataLakeGen2Filesystem&#34;, DataLakeGen2FilesystemArgs.builder()        
 *             .name(&#34;example&#34;)
 *             .storageAccountId(exampleAccount.id())
 *             .build());
 * 
 *         var exampleWorkspace = new Workspace(&#34;exampleWorkspace&#34;, WorkspaceArgs.builder()        
 *             .name(&#34;example&#34;)
 *             .resourceGroupName(example.name())
 *             .location(example.location())
 *             .storageDataLakeGen2FilesystemId(exampleDataLakeGen2Filesystem.id())
 *             .sqlAdministratorLogin(&#34;sqladminuser&#34;)
 *             .sqlAdministratorLoginPassword(&#34;H@Sh1CoR3!&#34;)
 *             .identity(WorkspaceIdentityArgs.builder()
 *                 .type(&#34;SystemAssigned&#34;)
 *                 .build())
 *             .build());
 * 
 *         var exampleSparkPool = new SparkPool(&#34;exampleSparkPool&#34;, SparkPoolArgs.builder()        
 *             .name(&#34;example&#34;)
 *             .synapseWorkspaceId(exampleWorkspace.id())
 *             .nodeSizeFamily(&#34;MemoryOptimized&#34;)
 *             .nodeSize(&#34;Small&#34;)
 *             .cacheSize(100)
 *             .autoScale(SparkPoolAutoScaleArgs.builder()
 *                 .maxNodeCount(50)
 *                 .minNodeCount(3)
 *                 .build())
 *             .autoPause(SparkPoolAutoPauseArgs.builder()
 *                 .delayInMinutes(15)
 *                 .build())
 *             .libraryRequirement(SparkPoolLibraryRequirementArgs.builder()
 *                 .content(&#34;&#34;&#34;
 * appnope==0.1.0
 * beautifulsoup4==4.6.3
 *                 &#34;&#34;&#34;)
 *                 .filename(&#34;requirements.txt&#34;)
 *                 .build())
 *             .sparkConfig(SparkPoolSparkConfigArgs.builder()
 *                 .content(&#34;&#34;&#34;
 * spark.shuffle.spill                true
 *                 &#34;&#34;&#34;)
 *                 .filename(&#34;config.txt&#34;)
 *                 .build())
 *             .tags(Map.of(&#34;ENV&#34;, &#34;Production&#34;))
 *             .build());
 * 
 *     }
 * }
 * ```
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Import
 * 
 * Synapse Spark Pool can be imported using the `resource id`, e.g.
 * 
 * ```sh
 * $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
 * ```
 * 
 */
@ResourceType(type="azure:synapse/sparkPool:SparkPool")
public class SparkPool extends com.pulumi.resources.CustomResource {
    /**
     * An `auto_pause` block as defined below.
     * 
     */
    @Export(name="autoPause", refs={SparkPoolAutoPause.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolAutoPause> autoPause;

    /**
     * @return An `auto_pause` block as defined below.
     * 
     */
    public Output<Optional<SparkPoolAutoPause>> autoPause() {
        return Codegen.optional(this.autoPause);
    }
    /**
     * An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    @Export(name="autoScale", refs={SparkPoolAutoScale.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolAutoScale> autoScale;

    /**
     * @return An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    public Output<Optional<SparkPoolAutoScale>> autoScale() {
        return Codegen.optional(this.autoScale);
    }
    /**
     * The cache size in the Spark Pool.
     * 
     */
    @Export(name="cacheSize", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> cacheSize;

    /**
     * @return The cache size in the Spark Pool.
     * 
     */
    public Output<Optional<Integer>> cacheSize() {
        return Codegen.optional(this.cacheSize);
    }
    /**
     * Indicates whether compute isolation is enabled or not. Defaults to `false`.
     * 
     */
    @Export(name="computeIsolationEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> computeIsolationEnabled;

    /**
     * @return Indicates whether compute isolation is enabled or not. Defaults to `false`.
     * 
     */
    public Output<Optional<Boolean>> computeIsolationEnabled() {
        return Codegen.optional(this.computeIsolationEnabled);
    }
    /**
     * Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
     * 
     */
    @Export(name="dynamicExecutorAllocationEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> dynamicExecutorAllocationEnabled;

    /**
     * @return Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
     * 
     */
    public Output<Optional<Boolean>> dynamicExecutorAllocationEnabled() {
        return Codegen.optional(this.dynamicExecutorAllocationEnabled);
    }
    /**
     * A `library_requirement` block as defined below.
     * 
     */
    @Export(name="libraryRequirement", refs={SparkPoolLibraryRequirement.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolLibraryRequirement> libraryRequirement;

    /**
     * @return A `library_requirement` block as defined below.
     * 
     */
    public Output<Optional<SparkPoolLibraryRequirement>> libraryRequirement() {
        return Codegen.optional(this.libraryRequirement);
    }
    /**
     * The maximum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.
     * 
     */
    @Export(name="maxExecutors", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> maxExecutors;

    /**
     * @return The maximum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.
     * 
     */
    public Output<Optional<Integer>> maxExecutors() {
        return Codegen.optional(this.maxExecutors);
    }
    /**
     * The minimum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.
     * 
     */
    @Export(name="minExecutors", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> minExecutors;

    /**
     * @return The minimum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.
     * 
     */
    public Output<Optional<Integer>> minExecutors() {
        return Codegen.optional(this.minExecutors);
    }
    /**
     * The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    /**
     * The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    @Export(name="nodeCount", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> nodeCount;

    /**
     * @return The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
     * 
     */
    public Output<Optional<Integer>> nodeCount() {
        return Codegen.optional(this.nodeCount);
    }
    /**
     * The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     * 
     */
    @Export(name="nodeSize", refs={String.class}, tree="[0]")
    private Output<String> nodeSize;

    /**
     * @return The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
     * 
     */
    public Output<String> nodeSize() {
        return this.nodeSize;
    }
    /**
     * The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     * 
     */
    @Export(name="nodeSizeFamily", refs={String.class}, tree="[0]")
    private Output<String> nodeSizeFamily;

    /**
     * @return The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
     * 
     */
    public Output<String> nodeSizeFamily() {
        return this.nodeSizeFamily;
    }
    /**
     * Indicates whether session level packages are enabled or not. Defaults to `false`.
     * 
     */
    @Export(name="sessionLevelPackagesEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> sessionLevelPackagesEnabled;

    /**
     * @return Indicates whether session level packages are enabled or not. Defaults to `false`.
     * 
     */
    public Output<Optional<Boolean>> sessionLevelPackagesEnabled() {
        return Codegen.optional(this.sessionLevelPackagesEnabled);
    }
    /**
     * A `spark_config` block as defined below.
     * 
     */
    @Export(name="sparkConfig", refs={SparkPoolSparkConfig.class}, tree="[0]")
    private Output</* @Nullable */ SparkPoolSparkConfig> sparkConfig;

    /**
     * @return A `spark_config` block as defined below.
     * 
     */
    public Output<Optional<SparkPoolSparkConfig>> sparkConfig() {
        return Codegen.optional(this.sparkConfig);
    }
    /**
     * The Spark events folder. Defaults to `/events`.
     * 
     */
    @Export(name="sparkEventsFolder", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkEventsFolder;

    /**
     * @return The Spark events folder. Defaults to `/events`.
     * 
     */
    public Output<Optional<String>> sparkEventsFolder() {
        return Codegen.optional(this.sparkEventsFolder);
    }
    /**
     * The default folder where Spark logs will be written. Defaults to `/logs`.
     * 
     */
    @Export(name="sparkLogFolder", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkLogFolder;

    /**
     * @return The default folder where Spark logs will be written. Defaults to `/logs`.
     * 
     */
    public Output<Optional<String>> sparkLogFolder() {
        return Codegen.optional(this.sparkLogFolder);
    }
    /**
     * The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2`, `3.3`, and `3.4`. Defaults to `2.4`.
     * 
     */
    @Export(name="sparkVersion", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sparkVersion;

    /**
     * @return The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2`, `3.3`, and `3.4`. Defaults to `2.4`.
     * 
     */
    public Output<Optional<String>> sparkVersion() {
        return Codegen.optional(this.sparkVersion);
    }
    /**
     * The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    @Export(name="synapseWorkspaceId", refs={String.class}, tree="[0]")
    private Output<String> synapseWorkspaceId;

    /**
     * @return The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
     * 
     */
    public Output<String> synapseWorkspaceId() {
        return this.synapseWorkspaceId;
    }
    /**
     * A mapping of tags which should be assigned to the Synapse Spark Pool.
     * 
     */
    @Export(name="tags", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> tags;

    /**
     * @return A mapping of tags which should be assigned to the Synapse Spark Pool.
     * 
     */
    public Output<Optional<Map<String,String>>> tags() {
        return Codegen.optional(this.tags);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public SparkPool(String name) {
        this(name, SparkPoolArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public SparkPool(String name, SparkPoolArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public SparkPool(String name, SparkPoolArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("azure:synapse/sparkPool:SparkPool", name, args == null ? SparkPoolArgs.Empty : args, makeResourceOptions(options, Codegen.empty()));
    }

    private SparkPool(String name, Output<String> id, @Nullable SparkPoolState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("azure:synapse/sparkPool:SparkPool", name, state, makeResourceOptions(options, id));
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static SparkPool get(String name, Output<String> id, @Nullable SparkPoolState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new SparkPool(name, id, state, options);
    }
}
