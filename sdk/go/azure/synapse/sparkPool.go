// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package synapse

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Manages a Synapse Spark Pool.
//
// ## Example Usage
//
// ```go
// package main
//
// import (
//
//	core/resourceGroup "github.com/pulumi/pulumi-azure/sdk/v1/go/azure/core/resourceGroup"
//	storage/account "github.com/pulumi/pulumi-azure/sdk/v1/go/azure/storage/account"
//	storage/dataLakeGen2Filesystem "github.com/pulumi/pulumi-azure/sdk/v1/go/azure/storage/dataLakeGen2Filesystem"
//	synapse/sparkPool "github.com/pulumi/pulumi-azure/sdk/v1/go/azure/synapse/sparkPool"
//	synapse/workspace "github.com/pulumi/pulumi-azure/sdk/v1/go/azure/synapse/workspace"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
// func main() {
// pulumi.Run(func(ctx *pulumi.Context) error {
// example, err := core/resourceGroup.NewResourceGroup(ctx, "example", &core/resourceGroup.ResourceGroupArgs{
// Name: "example-resources",
// Location: "West Europe",
// })
// if err != nil {
// return err
// }
// exampleAccount, err := storage/account.NewAccount(ctx, "example", &storage/account.AccountArgs{
// Name: "examplestorageacc",
// ResourceGroupName: example.Name,
// Location: example.Location,
// AccountTier: "Standard",
// AccountReplicationType: "LRS",
// AccountKind: "StorageV2",
// IsHnsEnabled: "true",
// })
// if err != nil {
// return err
// }
// exampleDataLakeGen2Filesystem, err := storage/dataLakeGen2Filesystem.NewDataLakeGen2Filesystem(ctx, "example", &storage/dataLakeGen2Filesystem.DataLakeGen2FilesystemArgs{
// Name: "example",
// StorageAccountId: exampleAccount.Id,
// })
// if err != nil {
// return err
// }
// exampleWorkspace, err := synapse/workspace.NewWorkspace(ctx, "example", &synapse/workspace.WorkspaceArgs{
// Name: "example",
// ResourceGroupName: example.Name,
// Location: example.Location,
// StorageDataLakeGen2FilesystemId: exampleDataLakeGen2Filesystem.Id,
// SqlAdministratorLogin: "sqladminuser",
// SqlAdministratorLoginPassword: "H@Sh1CoR3!",
// Identity: map[string]interface{}{
// "type": "SystemAssigned",
// },
// })
// if err != nil {
// return err
// }
// _, err = synapse/sparkPool.NewSparkPool(ctx, "example", &synapse/sparkPool.SparkPoolArgs{
// Name: "example",
// SynapseWorkspaceId: exampleWorkspace.Id,
// NodeSizeFamily: "MemoryOptimized",
// NodeSize: "Small",
// CacheSize: 100,
// AutoScale: map[string]interface{}{
// "maxNodeCount": 50,
// "minNodeCount": 3,
// },
// AutoPause: map[string]interface{}{
// "delayInMinutes": 15,
// },
// LibraryRequirement: map[string]interface{}{
// "content": "appnope==0.1.0\nbeautifulsoup4==4.6.3\n",
// "filename": "requirements.txt",
// },
// SparkConfig: map[string]interface{}{
// "content": "spark.shuffle.spill                true\n",
// "filename": "config.txt",
// },
// Tags: map[string]interface{}{
// "ENV": "Production",
// },
// })
// if err != nil {
// return err
// }
// return nil
// })
// }
// ```
//
// ## Import
//
// Synapse Spark Pool can be imported using the `resource id`, e.g.
//
// ```sh
// $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
// ```
type SparkPool struct {
	pulumi.CustomResourceState

	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrOutput `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrOutput `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrOutput `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled pulumi.BoolPtrOutput `pulumi:"computeIsolationEnabled"`
	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
	DynamicExecutorAllocationEnabled pulumi.BoolPtrOutput `pulumi:"dynamicExecutorAllocationEnabled"`
	// A `libraryRequirement` block as defined below.
	LibraryRequirement SparkPoolLibraryRequirementPtrOutput `pulumi:"libraryRequirement"`
	// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MaxExecutors pulumi.IntPtrOutput `pulumi:"maxExecutors"`
	// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MinExecutors pulumi.IntPtrOutput `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringOutput `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntPtrOutput `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringOutput `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily pulumi.StringOutput `pulumi:"nodeSizeFamily"`
	// Indicates whether session level packages are enabled or not. Defaults to `false`.
	SessionLevelPackagesEnabled pulumi.BoolPtrOutput `pulumi:"sessionLevelPackagesEnabled"`
	// A `sparkConfig` block as defined below.
	SparkConfig SparkPoolSparkConfigPtrOutput `pulumi:"sparkConfig"`
	// The Spark events folder. Defaults to `/events`.
	SparkEventsFolder pulumi.StringPtrOutput `pulumi:"sparkEventsFolder"`
	// The default folder where Spark logs will be written. Defaults to `/logs`.
	SparkLogFolder pulumi.StringPtrOutput `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
	SparkVersion pulumi.StringPtrOutput `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringOutput `pulumi:"synapseWorkspaceId"`
	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	Tags pulumi.StringMapOutput `pulumi:"tags"`
}

// NewSparkPool registers a new resource with the given unique name, arguments, and options.
func NewSparkPool(ctx *pulumi.Context,
	name string, args *SparkPoolArgs, opts ...pulumi.ResourceOption) (*SparkPool, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.NodeSize == nil {
		return nil, errors.New("invalid value for required argument 'NodeSize'")
	}
	if args.NodeSizeFamily == nil {
		return nil, errors.New("invalid value for required argument 'NodeSizeFamily'")
	}
	if args.SynapseWorkspaceId == nil {
		return nil, errors.New("invalid value for required argument 'SynapseWorkspaceId'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource SparkPool
	err := ctx.RegisterResource("azure:synapse/sparkPool:SparkPool", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetSparkPool gets an existing SparkPool resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetSparkPool(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *SparkPoolState, opts ...pulumi.ResourceOption) (*SparkPool, error) {
	var resource SparkPool
	err := ctx.ReadResource("azure:synapse/sparkPool:SparkPool", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering SparkPool resources.
type sparkPoolState struct {
	// An `autoPause` block as defined below.
	AutoPause *SparkPoolAutoPause `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale *SparkPoolAutoScale `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize *int `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled *bool `pulumi:"computeIsolationEnabled"`
	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
	DynamicExecutorAllocationEnabled *bool `pulumi:"dynamicExecutorAllocationEnabled"`
	// A `libraryRequirement` block as defined below.
	LibraryRequirement *SparkPoolLibraryRequirement `pulumi:"libraryRequirement"`
	// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MaxExecutors *int `pulumi:"maxExecutors"`
	// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MinExecutors *int `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name *string `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount *int `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize *string `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily *string `pulumi:"nodeSizeFamily"`
	// Indicates whether session level packages are enabled or not. Defaults to `false`.
	SessionLevelPackagesEnabled *bool `pulumi:"sessionLevelPackagesEnabled"`
	// A `sparkConfig` block as defined below.
	SparkConfig *SparkPoolSparkConfig `pulumi:"sparkConfig"`
	// The Spark events folder. Defaults to `/events`.
	SparkEventsFolder *string `pulumi:"sparkEventsFolder"`
	// The default folder where Spark logs will be written. Defaults to `/logs`.
	SparkLogFolder *string `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
	SparkVersion *string `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId *string `pulumi:"synapseWorkspaceId"`
	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	Tags map[string]string `pulumi:"tags"`
}

type SparkPoolState struct {
	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrInput
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrInput
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrInput
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled pulumi.BoolPtrInput
	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
	DynamicExecutorAllocationEnabled pulumi.BoolPtrInput
	// A `libraryRequirement` block as defined below.
	LibraryRequirement SparkPoolLibraryRequirementPtrInput
	// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MaxExecutors pulumi.IntPtrInput
	// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MinExecutors pulumi.IntPtrInput
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringPtrInput
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntPtrInput
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringPtrInput
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily pulumi.StringPtrInput
	// Indicates whether session level packages are enabled or not. Defaults to `false`.
	SessionLevelPackagesEnabled pulumi.BoolPtrInput
	// A `sparkConfig` block as defined below.
	SparkConfig SparkPoolSparkConfigPtrInput
	// The Spark events folder. Defaults to `/events`.
	SparkEventsFolder pulumi.StringPtrInput
	// The default folder where Spark logs will be written. Defaults to `/logs`.
	SparkLogFolder pulumi.StringPtrInput
	// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
	SparkVersion pulumi.StringPtrInput
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringPtrInput
	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	Tags pulumi.StringMapInput
}

func (SparkPoolState) ElementType() reflect.Type {
	return reflect.TypeOf((*sparkPoolState)(nil)).Elem()
}

type sparkPoolArgs struct {
	// An `autoPause` block as defined below.
	AutoPause *SparkPoolAutoPause `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale *SparkPoolAutoScale `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize *int `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled *bool `pulumi:"computeIsolationEnabled"`
	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
	DynamicExecutorAllocationEnabled *bool `pulumi:"dynamicExecutorAllocationEnabled"`
	// A `libraryRequirement` block as defined below.
	LibraryRequirement *SparkPoolLibraryRequirement `pulumi:"libraryRequirement"`
	// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MaxExecutors *int `pulumi:"maxExecutors"`
	// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MinExecutors *int `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name *string `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount *int `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize string `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily string `pulumi:"nodeSizeFamily"`
	// Indicates whether session level packages are enabled or not. Defaults to `false`.
	SessionLevelPackagesEnabled *bool `pulumi:"sessionLevelPackagesEnabled"`
	// A `sparkConfig` block as defined below.
	SparkConfig *SparkPoolSparkConfig `pulumi:"sparkConfig"`
	// The Spark events folder. Defaults to `/events`.
	SparkEventsFolder *string `pulumi:"sparkEventsFolder"`
	// The default folder where Spark logs will be written. Defaults to `/logs`.
	SparkLogFolder *string `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
	SparkVersion *string `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId string `pulumi:"synapseWorkspaceId"`
	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	Tags map[string]string `pulumi:"tags"`
}

// The set of arguments for constructing a SparkPool resource.
type SparkPoolArgs struct {
	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrInput
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrInput
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrInput
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled pulumi.BoolPtrInput
	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
	DynamicExecutorAllocationEnabled pulumi.BoolPtrInput
	// A `libraryRequirement` block as defined below.
	LibraryRequirement SparkPoolLibraryRequirementPtrInput
	// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MaxExecutors pulumi.IntPtrInput
	// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
	MinExecutors pulumi.IntPtrInput
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringPtrInput
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntPtrInput
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringInput
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily pulumi.StringInput
	// Indicates whether session level packages are enabled or not. Defaults to `false`.
	SessionLevelPackagesEnabled pulumi.BoolPtrInput
	// A `sparkConfig` block as defined below.
	SparkConfig SparkPoolSparkConfigPtrInput
	// The Spark events folder. Defaults to `/events`.
	SparkEventsFolder pulumi.StringPtrInput
	// The default folder where Spark logs will be written. Defaults to `/logs`.
	SparkLogFolder pulumi.StringPtrInput
	// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
	SparkVersion pulumi.StringPtrInput
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringInput
	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	Tags pulumi.StringMapInput
}

func (SparkPoolArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*sparkPoolArgs)(nil)).Elem()
}

type SparkPoolInput interface {
	pulumi.Input

	ToSparkPoolOutput() SparkPoolOutput
	ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput
}

func (*SparkPool) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkPool)(nil)).Elem()
}

func (i *SparkPool) ToSparkPoolOutput() SparkPoolOutput {
	return i.ToSparkPoolOutputWithContext(context.Background())
}

func (i *SparkPool) ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolOutput)
}

// SparkPoolArrayInput is an input type that accepts SparkPoolArray and SparkPoolArrayOutput values.
// You can construct a concrete instance of `SparkPoolArrayInput` via:
//
//	SparkPoolArray{ SparkPoolArgs{...} }
type SparkPoolArrayInput interface {
	pulumi.Input

	ToSparkPoolArrayOutput() SparkPoolArrayOutput
	ToSparkPoolArrayOutputWithContext(context.Context) SparkPoolArrayOutput
}

type SparkPoolArray []SparkPoolInput

func (SparkPoolArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*SparkPool)(nil)).Elem()
}

func (i SparkPoolArray) ToSparkPoolArrayOutput() SparkPoolArrayOutput {
	return i.ToSparkPoolArrayOutputWithContext(context.Background())
}

func (i SparkPoolArray) ToSparkPoolArrayOutputWithContext(ctx context.Context) SparkPoolArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolArrayOutput)
}

// SparkPoolMapInput is an input type that accepts SparkPoolMap and SparkPoolMapOutput values.
// You can construct a concrete instance of `SparkPoolMapInput` via:
//
//	SparkPoolMap{ "key": SparkPoolArgs{...} }
type SparkPoolMapInput interface {
	pulumi.Input

	ToSparkPoolMapOutput() SparkPoolMapOutput
	ToSparkPoolMapOutputWithContext(context.Context) SparkPoolMapOutput
}

type SparkPoolMap map[string]SparkPoolInput

func (SparkPoolMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*SparkPool)(nil)).Elem()
}

func (i SparkPoolMap) ToSparkPoolMapOutput() SparkPoolMapOutput {
	return i.ToSparkPoolMapOutputWithContext(context.Background())
}

func (i SparkPoolMap) ToSparkPoolMapOutputWithContext(ctx context.Context) SparkPoolMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolMapOutput)
}

type SparkPoolOutput struct{ *pulumi.OutputState }

func (SparkPoolOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkPool)(nil)).Elem()
}

func (o SparkPoolOutput) ToSparkPoolOutput() SparkPoolOutput {
	return o
}

func (o SparkPoolOutput) ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput {
	return o
}

// An `autoPause` block as defined below.
func (o SparkPoolOutput) AutoPause() SparkPoolAutoPausePtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolAutoPausePtrOutput { return v.AutoPause }).(SparkPoolAutoPausePtrOutput)
}

// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
func (o SparkPoolOutput) AutoScale() SparkPoolAutoScalePtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolAutoScalePtrOutput { return v.AutoScale }).(SparkPoolAutoScalePtrOutput)
}

// The cache size in the Spark Pool.
func (o SparkPoolOutput) CacheSize() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.CacheSize }).(pulumi.IntPtrOutput)
}

// Indicates whether compute isolation is enabled or not. Defaults to `false`.
func (o SparkPoolOutput) ComputeIsolationEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.ComputeIsolationEnabled }).(pulumi.BoolPtrOutput)
}

// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.
func (o SparkPoolOutput) DynamicExecutorAllocationEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.DynamicExecutorAllocationEnabled }).(pulumi.BoolPtrOutput)
}

// A `libraryRequirement` block as defined below.
func (o SparkPoolOutput) LibraryRequirement() SparkPoolLibraryRequirementPtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolLibraryRequirementPtrOutput { return v.LibraryRequirement }).(SparkPoolLibraryRequirementPtrOutput)
}

// The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
func (o SparkPoolOutput) MaxExecutors() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.MaxExecutors }).(pulumi.IntPtrOutput)
}

// The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
func (o SparkPoolOutput) MinExecutors() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.MinExecutors }).(pulumi.IntPtrOutput)
}

// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
func (o SparkPoolOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
func (o SparkPoolOutput) NodeCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.NodeCount }).(pulumi.IntPtrOutput)
}

// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
func (o SparkPoolOutput) NodeSize() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.NodeSize }).(pulumi.StringOutput)
}

// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
func (o SparkPoolOutput) NodeSizeFamily() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.NodeSizeFamily }).(pulumi.StringOutput)
}

// Indicates whether session level packages are enabled or not. Defaults to `false`.
func (o SparkPoolOutput) SessionLevelPackagesEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.SessionLevelPackagesEnabled }).(pulumi.BoolPtrOutput)
}

// A `sparkConfig` block as defined below.
func (o SparkPoolOutput) SparkConfig() SparkPoolSparkConfigPtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolSparkConfigPtrOutput { return v.SparkConfig }).(SparkPoolSparkConfigPtrOutput)
}

// The Spark events folder. Defaults to `/events`.
func (o SparkPoolOutput) SparkEventsFolder() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringPtrOutput { return v.SparkEventsFolder }).(pulumi.StringPtrOutput)
}

// The default folder where Spark logs will be written. Defaults to `/logs`.
func (o SparkPoolOutput) SparkLogFolder() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringPtrOutput { return v.SparkLogFolder }).(pulumi.StringPtrOutput)
}

// The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.
func (o SparkPoolOutput) SparkVersion() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringPtrOutput { return v.SparkVersion }).(pulumi.StringPtrOutput)
}

// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
func (o SparkPoolOutput) SynapseWorkspaceId() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.SynapseWorkspaceId }).(pulumi.StringOutput)
}

// A mapping of tags which should be assigned to the Synapse Spark Pool.
func (o SparkPoolOutput) Tags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringMapOutput { return v.Tags }).(pulumi.StringMapOutput)
}

type SparkPoolArrayOutput struct{ *pulumi.OutputState }

func (SparkPoolArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*SparkPool)(nil)).Elem()
}

func (o SparkPoolArrayOutput) ToSparkPoolArrayOutput() SparkPoolArrayOutput {
	return o
}

func (o SparkPoolArrayOutput) ToSparkPoolArrayOutputWithContext(ctx context.Context) SparkPoolArrayOutput {
	return o
}

func (o SparkPoolArrayOutput) Index(i pulumi.IntInput) SparkPoolOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *SparkPool {
		return vs[0].([]*SparkPool)[vs[1].(int)]
	}).(SparkPoolOutput)
}

type SparkPoolMapOutput struct{ *pulumi.OutputState }

func (SparkPoolMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*SparkPool)(nil)).Elem()
}

func (o SparkPoolMapOutput) ToSparkPoolMapOutput() SparkPoolMapOutput {
	return o
}

func (o SparkPoolMapOutput) ToSparkPoolMapOutputWithContext(ctx context.Context) SparkPoolMapOutput {
	return o
}

func (o SparkPoolMapOutput) MapIndex(k pulumi.StringInput) SparkPoolOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *SparkPool {
		return vs[0].(map[string]*SparkPool)[vs[1].(string)]
	}).(SparkPoolOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolInput)(nil)).Elem(), &SparkPool{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolArrayInput)(nil)).Elem(), SparkPoolArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolMapInput)(nil)).Elem(), SparkPoolMap{})
	pulumi.RegisterOutputType(SparkPoolOutput{})
	pulumi.RegisterOutputType(SparkPoolArrayOutput{})
	pulumi.RegisterOutputType(SparkPoolMapOutput{})
}
