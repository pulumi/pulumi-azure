// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package synapse

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-azure/sdk/v6/go/azure/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Manages a Synapse Spark Pool.
//
// ## Example Usage
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-azure/sdk/v6/go/azure/core"
//	"github.com/pulumi/pulumi-azure/sdk/v6/go/azure/storage"
//	"github.com/pulumi/pulumi-azure/sdk/v6/go/azure/synapse"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			example, err := core.NewResourceGroup(ctx, "example", &core.ResourceGroupArgs{
//				Name:     pulumi.String("example-resources"),
//				Location: pulumi.String("West Europe"),
//			})
//			if err != nil {
//				return err
//			}
//			exampleAccount, err := storage.NewAccount(ctx, "example", &storage.AccountArgs{
//				Name:                   pulumi.String("examplestorageacc"),
//				ResourceGroupName:      example.Name,
//				Location:               example.Location,
//				AccountTier:            pulumi.String("Standard"),
//				AccountReplicationType: pulumi.String("LRS"),
//				AccountKind:            pulumi.String("StorageV2"),
//				IsHnsEnabled:           pulumi.Bool(true),
//			})
//			if err != nil {
//				return err
//			}
//			exampleDataLakeGen2Filesystem, err := storage.NewDataLakeGen2Filesystem(ctx, "example", &storage.DataLakeGen2FilesystemArgs{
//				Name:             pulumi.String("example"),
//				StorageAccountId: exampleAccount.ID(),
//			})
//			if err != nil {
//				return err
//			}
//			exampleWorkspace, err := synapse.NewWorkspace(ctx, "example", &synapse.WorkspaceArgs{
//				Name:                            pulumi.String("example"),
//				ResourceGroupName:               example.Name,
//				Location:                        example.Location,
//				StorageDataLakeGen2FilesystemId: exampleDataLakeGen2Filesystem.ID(),
//				SqlAdministratorLogin:           pulumi.String("sqladminuser"),
//				SqlAdministratorLoginPassword:   pulumi.String("H@Sh1CoR3!"),
//				Identity: &synapse.WorkspaceIdentityArgs{
//					Type: pulumi.String("SystemAssigned"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			_, err = synapse.NewSparkPool(ctx, "example", &synapse.SparkPoolArgs{
//				Name:               pulumi.String("example"),
//				SynapseWorkspaceId: exampleWorkspace.ID(),
//				NodeSizeFamily:     pulumi.String("MemoryOptimized"),
//				NodeSize:           pulumi.String("Small"),
//				CacheSize:          pulumi.Int(100),
//				AutoScale: &synapse.SparkPoolAutoScaleArgs{
//					MaxNodeCount: pulumi.Int(50),
//					MinNodeCount: pulumi.Int(3),
//				},
//				AutoPause: &synapse.SparkPoolAutoPauseArgs{
//					DelayInMinutes: pulumi.Int(15),
//				},
//				LibraryRequirement: &synapse.SparkPoolLibraryRequirementArgs{
//					Content:  pulumi.String("appnope==0.1.0\nbeautifulsoup4==4.6.3\n"),
//					Filename: pulumi.String("requirements.txt"),
//				},
//				SparkConfig: &synapse.SparkPoolSparkConfigArgs{
//					Content:  pulumi.String("spark.shuffle.spill                true\n"),
//					Filename: pulumi.String("config.txt"),
//				},
//				SparkVersion: pulumi.String("3.2"),
//				Tags: pulumi.StringMap{
//					"ENV": pulumi.String("Production"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Import
//
// Synapse Spark Pool can be imported using the `resource id`, e.g.
//
// ```sh
// $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
// ```
type SparkPool struct {
	pulumi.CustomResourceState

	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrOutput `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrOutput `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrOutput `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled          pulumi.BoolPtrOutput                 `pulumi:"computeIsolationEnabled"`
	DynamicExecutorAllocationEnabled pulumi.BoolPtrOutput                 `pulumi:"dynamicExecutorAllocationEnabled"`
	LibraryRequirement               SparkPoolLibraryRequirementPtrOutput `pulumi:"libraryRequirement"`
	MaxExecutors                     pulumi.IntPtrOutput                  `pulumi:"maxExecutors"`
	MinExecutors                     pulumi.IntPtrOutput                  `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringOutput `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntOutput `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringOutput `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily              pulumi.StringOutput           `pulumi:"nodeSizeFamily"`
	SessionLevelPackagesEnabled pulumi.BoolPtrOutput          `pulumi:"sessionLevelPackagesEnabled"`
	SparkConfig                 SparkPoolSparkConfigPtrOutput `pulumi:"sparkConfig"`
	SparkEventsFolder           pulumi.StringPtrOutput        `pulumi:"sparkEventsFolder"`
	SparkLogFolder              pulumi.StringPtrOutput        `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
	SparkVersion pulumi.StringOutput `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringOutput    `pulumi:"synapseWorkspaceId"`
	Tags               pulumi.StringMapOutput `pulumi:"tags"`
}

// NewSparkPool registers a new resource with the given unique name, arguments, and options.
func NewSparkPool(ctx *pulumi.Context,
	name string, args *SparkPoolArgs, opts ...pulumi.ResourceOption) (*SparkPool, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.NodeSize == nil {
		return nil, errors.New("invalid value for required argument 'NodeSize'")
	}
	if args.NodeSizeFamily == nil {
		return nil, errors.New("invalid value for required argument 'NodeSizeFamily'")
	}
	if args.SparkVersion == nil {
		return nil, errors.New("invalid value for required argument 'SparkVersion'")
	}
	if args.SynapseWorkspaceId == nil {
		return nil, errors.New("invalid value for required argument 'SynapseWorkspaceId'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource SparkPool
	err := ctx.RegisterResource("azure:synapse/sparkPool:SparkPool", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetSparkPool gets an existing SparkPool resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetSparkPool(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *SparkPoolState, opts ...pulumi.ResourceOption) (*SparkPool, error) {
	var resource SparkPool
	err := ctx.ReadResource("azure:synapse/sparkPool:SparkPool", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering SparkPool resources.
type sparkPoolState struct {
	// An `autoPause` block as defined below.
	AutoPause *SparkPoolAutoPause `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale *SparkPoolAutoScale `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize *int `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled          *bool                        `pulumi:"computeIsolationEnabled"`
	DynamicExecutorAllocationEnabled *bool                        `pulumi:"dynamicExecutorAllocationEnabled"`
	LibraryRequirement               *SparkPoolLibraryRequirement `pulumi:"libraryRequirement"`
	MaxExecutors                     *int                         `pulumi:"maxExecutors"`
	MinExecutors                     *int                         `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name *string `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount *int `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize *string `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily              *string               `pulumi:"nodeSizeFamily"`
	SessionLevelPackagesEnabled *bool                 `pulumi:"sessionLevelPackagesEnabled"`
	SparkConfig                 *SparkPoolSparkConfig `pulumi:"sparkConfig"`
	SparkEventsFolder           *string               `pulumi:"sparkEventsFolder"`
	SparkLogFolder              *string               `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
	SparkVersion *string `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId *string           `pulumi:"synapseWorkspaceId"`
	Tags               map[string]string `pulumi:"tags"`
}

type SparkPoolState struct {
	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrInput
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrInput
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrInput
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled          pulumi.BoolPtrInput
	DynamicExecutorAllocationEnabled pulumi.BoolPtrInput
	LibraryRequirement               SparkPoolLibraryRequirementPtrInput
	MaxExecutors                     pulumi.IntPtrInput
	MinExecutors                     pulumi.IntPtrInput
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringPtrInput
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntPtrInput
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringPtrInput
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily              pulumi.StringPtrInput
	SessionLevelPackagesEnabled pulumi.BoolPtrInput
	SparkConfig                 SparkPoolSparkConfigPtrInput
	SparkEventsFolder           pulumi.StringPtrInput
	SparkLogFolder              pulumi.StringPtrInput
	// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
	SparkVersion pulumi.StringPtrInput
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringPtrInput
	Tags               pulumi.StringMapInput
}

func (SparkPoolState) ElementType() reflect.Type {
	return reflect.TypeOf((*sparkPoolState)(nil)).Elem()
}

type sparkPoolArgs struct {
	// An `autoPause` block as defined below.
	AutoPause *SparkPoolAutoPause `pulumi:"autoPause"`
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale *SparkPoolAutoScale `pulumi:"autoScale"`
	// The cache size in the Spark Pool.
	CacheSize *int `pulumi:"cacheSize"`
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled          *bool                        `pulumi:"computeIsolationEnabled"`
	DynamicExecutorAllocationEnabled *bool                        `pulumi:"dynamicExecutorAllocationEnabled"`
	LibraryRequirement               *SparkPoolLibraryRequirement `pulumi:"libraryRequirement"`
	MaxExecutors                     *int                         `pulumi:"maxExecutors"`
	MinExecutors                     *int                         `pulumi:"minExecutors"`
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name *string `pulumi:"name"`
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount *int `pulumi:"nodeCount"`
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize string `pulumi:"nodeSize"`
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily              string                `pulumi:"nodeSizeFamily"`
	SessionLevelPackagesEnabled *bool                 `pulumi:"sessionLevelPackagesEnabled"`
	SparkConfig                 *SparkPoolSparkConfig `pulumi:"sparkConfig"`
	SparkEventsFolder           *string               `pulumi:"sparkEventsFolder"`
	SparkLogFolder              *string               `pulumi:"sparkLogFolder"`
	// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
	SparkVersion string `pulumi:"sparkVersion"`
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId string            `pulumi:"synapseWorkspaceId"`
	Tags               map[string]string `pulumi:"tags"`
}

// The set of arguments for constructing a SparkPool resource.
type SparkPoolArgs struct {
	// An `autoPause` block as defined below.
	AutoPause SparkPoolAutoPausePtrInput
	// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
	AutoScale SparkPoolAutoScalePtrInput
	// The cache size in the Spark Pool.
	CacheSize pulumi.IntPtrInput
	// Indicates whether compute isolation is enabled or not. Defaults to `false`.
	ComputeIsolationEnabled          pulumi.BoolPtrInput
	DynamicExecutorAllocationEnabled pulumi.BoolPtrInput
	LibraryRequirement               SparkPoolLibraryRequirementPtrInput
	MaxExecutors                     pulumi.IntPtrInput
	MinExecutors                     pulumi.IntPtrInput
	// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
	Name pulumi.StringPtrInput
	// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
	NodeCount pulumi.IntPtrInput
	// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
	NodeSize pulumi.StringInput
	// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
	NodeSizeFamily              pulumi.StringInput
	SessionLevelPackagesEnabled pulumi.BoolPtrInput
	SparkConfig                 SparkPoolSparkConfigPtrInput
	SparkEventsFolder           pulumi.StringPtrInput
	SparkLogFolder              pulumi.StringPtrInput
	// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
	SparkVersion pulumi.StringInput
	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	SynapseWorkspaceId pulumi.StringInput
	Tags               pulumi.StringMapInput
}

func (SparkPoolArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*sparkPoolArgs)(nil)).Elem()
}

type SparkPoolInput interface {
	pulumi.Input

	ToSparkPoolOutput() SparkPoolOutput
	ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput
}

func (*SparkPool) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkPool)(nil)).Elem()
}

func (i *SparkPool) ToSparkPoolOutput() SparkPoolOutput {
	return i.ToSparkPoolOutputWithContext(context.Background())
}

func (i *SparkPool) ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolOutput)
}

// SparkPoolArrayInput is an input type that accepts SparkPoolArray and SparkPoolArrayOutput values.
// You can construct a concrete instance of `SparkPoolArrayInput` via:
//
//	SparkPoolArray{ SparkPoolArgs{...} }
type SparkPoolArrayInput interface {
	pulumi.Input

	ToSparkPoolArrayOutput() SparkPoolArrayOutput
	ToSparkPoolArrayOutputWithContext(context.Context) SparkPoolArrayOutput
}

type SparkPoolArray []SparkPoolInput

func (SparkPoolArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*SparkPool)(nil)).Elem()
}

func (i SparkPoolArray) ToSparkPoolArrayOutput() SparkPoolArrayOutput {
	return i.ToSparkPoolArrayOutputWithContext(context.Background())
}

func (i SparkPoolArray) ToSparkPoolArrayOutputWithContext(ctx context.Context) SparkPoolArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolArrayOutput)
}

// SparkPoolMapInput is an input type that accepts SparkPoolMap and SparkPoolMapOutput values.
// You can construct a concrete instance of `SparkPoolMapInput` via:
//
//	SparkPoolMap{ "key": SparkPoolArgs{...} }
type SparkPoolMapInput interface {
	pulumi.Input

	ToSparkPoolMapOutput() SparkPoolMapOutput
	ToSparkPoolMapOutputWithContext(context.Context) SparkPoolMapOutput
}

type SparkPoolMap map[string]SparkPoolInput

func (SparkPoolMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*SparkPool)(nil)).Elem()
}

func (i SparkPoolMap) ToSparkPoolMapOutput() SparkPoolMapOutput {
	return i.ToSparkPoolMapOutputWithContext(context.Background())
}

func (i SparkPoolMap) ToSparkPoolMapOutputWithContext(ctx context.Context) SparkPoolMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkPoolMapOutput)
}

type SparkPoolOutput struct{ *pulumi.OutputState }

func (SparkPoolOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkPool)(nil)).Elem()
}

func (o SparkPoolOutput) ToSparkPoolOutput() SparkPoolOutput {
	return o
}

func (o SparkPoolOutput) ToSparkPoolOutputWithContext(ctx context.Context) SparkPoolOutput {
	return o
}

// An `autoPause` block as defined below.
func (o SparkPoolOutput) AutoPause() SparkPoolAutoPausePtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolAutoPausePtrOutput { return v.AutoPause }).(SparkPoolAutoPausePtrOutput)
}

// An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.
func (o SparkPoolOutput) AutoScale() SparkPoolAutoScalePtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolAutoScalePtrOutput { return v.AutoScale }).(SparkPoolAutoScalePtrOutput)
}

// The cache size in the Spark Pool.
func (o SparkPoolOutput) CacheSize() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.CacheSize }).(pulumi.IntPtrOutput)
}

// Indicates whether compute isolation is enabled or not. Defaults to `false`.
func (o SparkPoolOutput) ComputeIsolationEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.ComputeIsolationEnabled }).(pulumi.BoolPtrOutput)
}

func (o SparkPoolOutput) DynamicExecutorAllocationEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.DynamicExecutorAllocationEnabled }).(pulumi.BoolPtrOutput)
}

func (o SparkPoolOutput) LibraryRequirement() SparkPoolLibraryRequirementPtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolLibraryRequirementPtrOutput { return v.LibraryRequirement }).(SparkPoolLibraryRequirementPtrOutput)
}

func (o SparkPoolOutput) MaxExecutors() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.MaxExecutors }).(pulumi.IntPtrOutput)
}

func (o SparkPoolOutput) MinExecutors() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntPtrOutput { return v.MinExecutors }).(pulumi.IntPtrOutput)
}

// The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
func (o SparkPoolOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.
func (o SparkPoolOutput) NodeCount() pulumi.IntOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.IntOutput { return v.NodeCount }).(pulumi.IntOutput)
}

// The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.
func (o SparkPoolOutput) NodeSize() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.NodeSize }).(pulumi.StringOutput)
}

// The kind of nodes that the Spark Pool provides. Possible values are `HardwareAcceleratedFPGA`, `HardwareAcceleratedGPU`, `MemoryOptimized`, and `None`.
func (o SparkPoolOutput) NodeSizeFamily() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.NodeSizeFamily }).(pulumi.StringOutput)
}

func (o SparkPoolOutput) SessionLevelPackagesEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.BoolPtrOutput { return v.SessionLevelPackagesEnabled }).(pulumi.BoolPtrOutput)
}

func (o SparkPoolOutput) SparkConfig() SparkPoolSparkConfigPtrOutput {
	return o.ApplyT(func(v *SparkPool) SparkPoolSparkConfigPtrOutput { return v.SparkConfig }).(SparkPoolSparkConfigPtrOutput)
}

func (o SparkPoolOutput) SparkEventsFolder() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringPtrOutput { return v.SparkEventsFolder }).(pulumi.StringPtrOutput)
}

func (o SparkPoolOutput) SparkLogFolder() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringPtrOutput { return v.SparkLogFolder }).(pulumi.StringPtrOutput)
}

// The Apache Spark version. Possible values are `3.2`, `3.3`, and `3.4`.
func (o SparkPoolOutput) SparkVersion() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.SparkVersion }).(pulumi.StringOutput)
}

// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
func (o SparkPoolOutput) SynapseWorkspaceId() pulumi.StringOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringOutput { return v.SynapseWorkspaceId }).(pulumi.StringOutput)
}

func (o SparkPoolOutput) Tags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkPool) pulumi.StringMapOutput { return v.Tags }).(pulumi.StringMapOutput)
}

type SparkPoolArrayOutput struct{ *pulumi.OutputState }

func (SparkPoolArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*SparkPool)(nil)).Elem()
}

func (o SparkPoolArrayOutput) ToSparkPoolArrayOutput() SparkPoolArrayOutput {
	return o
}

func (o SparkPoolArrayOutput) ToSparkPoolArrayOutputWithContext(ctx context.Context) SparkPoolArrayOutput {
	return o
}

func (o SparkPoolArrayOutput) Index(i pulumi.IntInput) SparkPoolOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *SparkPool {
		return vs[0].([]*SparkPool)[vs[1].(int)]
	}).(SparkPoolOutput)
}

type SparkPoolMapOutput struct{ *pulumi.OutputState }

func (SparkPoolMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*SparkPool)(nil)).Elem()
}

func (o SparkPoolMapOutput) ToSparkPoolMapOutput() SparkPoolMapOutput {
	return o
}

func (o SparkPoolMapOutput) ToSparkPoolMapOutputWithContext(ctx context.Context) SparkPoolMapOutput {
	return o
}

func (o SparkPoolMapOutput) MapIndex(k pulumi.StringInput) SparkPoolOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *SparkPool {
		return vs[0].(map[string]*SparkPool)[vs[1].(string)]
	}).(SparkPoolOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolInput)(nil)).Elem(), &SparkPool{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolArrayInput)(nil)).Elem(), SparkPoolArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkPoolMapInput)(nil)).Elem(), SparkPoolMap{})
	pulumi.RegisterOutputType(SparkPoolOutput{})
	pulumi.RegisterOutputType(SparkPoolArrayOutput{})
	pulumi.RegisterOutputType(SparkPoolMapOutput{})
}
