// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Azure.DataFactory
{
    /// <summary>
    /// Manages a Data Flow inside an Azure Data Factory.
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Azure = Pulumi.Azure;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var example = new Azure.Core.ResourceGroup("example", new()
    ///     {
    ///         Name = "example-resources",
    ///         Location = "West Europe",
    ///     });
    /// 
    ///     var exampleAccount = new Azure.Storage.Account("example", new()
    ///     {
    ///         Name = "example",
    ///         Location = example.Location,
    ///         ResourceGroupName = example.Name,
    ///         AccountTier = "Standard",
    ///         AccountReplicationType = "LRS",
    ///     });
    /// 
    ///     var exampleFactory = new Azure.DataFactory.Factory("example", new()
    ///     {
    ///         Name = "example",
    ///         Location = example.Location,
    ///         ResourceGroupName = example.Name,
    ///     });
    /// 
    ///     var exampleLinkedCustomService = new Azure.DataFactory.LinkedCustomService("example", new()
    ///     {
    ///         Name = "linked_service",
    ///         DataFactoryId = exampleFactory.Id,
    ///         Type = "AzureBlobStorage",
    ///         TypePropertiesJson = exampleAccount.PrimaryConnectionString.Apply(primaryConnectionString =&gt; @$"{{
    ///   \""connectionString\"": \""{primaryConnectionString}\""
    /// }}
    /// "),
    ///     });
    /// 
    ///     var example1 = new Azure.DataFactory.DatasetJson("example1", new()
    ///     {
    ///         Name = "dataset1",
    ///         DataFactoryId = exampleFactory.Id,
    ///         LinkedServiceName = exampleLinkedCustomService.Name,
    ///         AzureBlobStorageLocation = new Azure.DataFactory.Inputs.DatasetJsonAzureBlobStorageLocationArgs
    ///         {
    ///             Container = "container",
    ///             Path = "foo/bar/",
    ///             Filename = "foo.txt",
    ///         },
    ///         Encoding = "UTF-8",
    ///     });
    /// 
    ///     var example2 = new Azure.DataFactory.DatasetJson("example2", new()
    ///     {
    ///         Name = "dataset2",
    ///         DataFactoryId = exampleFactory.Id,
    ///         LinkedServiceName = exampleLinkedCustomService.Name,
    ///         AzureBlobStorageLocation = new Azure.DataFactory.Inputs.DatasetJsonAzureBlobStorageLocationArgs
    ///         {
    ///             Container = "container",
    ///             Path = "foo/bar/",
    ///             Filename = "bar.txt",
    ///         },
    ///         Encoding = "UTF-8",
    ///     });
    /// 
    ///     var example1FlowletDataFlow = new Azure.DataFactory.FlowletDataFlow("example1", new()
    ///     {
    ///         Name = "example",
    ///         DataFactoryId = exampleFactory.Id,
    ///         Sources = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.FlowletDataFlowSourceArgs
    ///             {
    ///                 Name = "source1",
    ///                 LinkedService = new Azure.DataFactory.Inputs.FlowletDataFlowSourceLinkedServiceArgs
    ///                 {
    ///                     Name = exampleLinkedCustomService.Name,
    ///                 },
    ///             },
    ///         },
    ///         Sinks = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.FlowletDataFlowSinkArgs
    ///             {
    ///                 Name = "sink1",
    ///                 LinkedService = new Azure.DataFactory.Inputs.FlowletDataFlowSinkLinkedServiceArgs
    ///                 {
    ///                     Name = exampleLinkedCustomService.Name,
    ///                 },
    ///             },
    ///         },
    ///         Script = @"source(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   limit: 100, 
    ///   ignoreNoFilesFound: false, 
    ///   documentForm: 'documentPerLine') ~&gt; source1 
    /// source1 sink(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   skipDuplicateMapInputs: true, 
    ///   skipDuplicateMapOutputs: true) ~&gt; sink1
    /// ",
    ///     });
    /// 
    ///     var example2FlowletDataFlow = new Azure.DataFactory.FlowletDataFlow("example2", new()
    ///     {
    ///         Name = "example",
    ///         DataFactoryId = exampleFactory.Id,
    ///         Sources = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.FlowletDataFlowSourceArgs
    ///             {
    ///                 Name = "source1",
    ///                 LinkedService = new Azure.DataFactory.Inputs.FlowletDataFlowSourceLinkedServiceArgs
    ///                 {
    ///                     Name = exampleLinkedCustomService.Name,
    ///                 },
    ///             },
    ///         },
    ///         Sinks = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.FlowletDataFlowSinkArgs
    ///             {
    ///                 Name = "sink1",
    ///                 LinkedService = new Azure.DataFactory.Inputs.FlowletDataFlowSinkLinkedServiceArgs
    ///                 {
    ///                     Name = exampleLinkedCustomService.Name,
    ///                 },
    ///             },
    ///         },
    ///         Script = @"source(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   limit: 100, 
    ///   ignoreNoFilesFound: false, 
    ///   documentForm: 'documentPerLine') ~&gt; source1 
    /// source1 sink(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   skipDuplicateMapInputs: true, 
    ///   skipDuplicateMapOutputs: true) ~&gt; sink1
    /// ",
    ///     });
    /// 
    ///     var exampleDataFlow = new Azure.DataFactory.DataFlow("example", new()
    ///     {
    ///         Name = "example",
    ///         DataFactoryId = exampleFactory.Id,
    ///         Sources = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.DataFlowSourceArgs
    ///             {
    ///                 Name = "source1",
    ///                 Flowlet = new Azure.DataFactory.Inputs.DataFlowSourceFlowletArgs
    ///                 {
    ///                     Name = example1FlowletDataFlow.Name,
    ///                     Parameters = 
    ///                     {
    ///                         { "Key1", "value1" },
    ///                     },
    ///                 },
    ///                 Dataset = new Azure.DataFactory.Inputs.DataFlowSourceDatasetArgs
    ///                 {
    ///                     Name = example1.Name,
    ///                 },
    ///             },
    ///         },
    ///         Sinks = new[]
    ///         {
    ///             new Azure.DataFactory.Inputs.DataFlowSinkArgs
    ///             {
    ///                 Name = "sink1",
    ///                 Flowlet = new Azure.DataFactory.Inputs.DataFlowSinkFlowletArgs
    ///                 {
    ///                     Name = example2FlowletDataFlow.Name,
    ///                     Parameters = 
    ///                     {
    ///                         { "Key1", "value1" },
    ///                     },
    ///                 },
    ///                 Dataset = new Azure.DataFactory.Inputs.DataFlowSinkDatasetArgs
    ///                 {
    ///                     Name = example2.Name,
    ///                 },
    ///             },
    ///         },
    ///         Script = @"source(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   limit: 100, 
    ///   ignoreNoFilesFound: false, 
    ///   documentForm: 'documentPerLine') ~&gt; source1 
    /// source1 sink(
    ///   allowSchemaDrift: true, 
    ///   validateSchema: false, 
    ///   skipDuplicateMapInputs: true, 
    ///   skipDuplicateMapOutputs: true) ~&gt; sink1
    /// ",
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## API Providers
    /// 
    /// &lt;!-- This section is generated, changes will be overwritten --&gt;
    /// This resource uses the following Azure API Providers:
    /// 
    /// * `Microsoft.DataFactory` - 2018-06-01
    /// 
    /// ## Import
    /// 
    /// Data Factory Data Flow can be imported using the `resource id`, e.g.
    /// 
    /// ```sh
    /// $ pulumi import azure:datafactory/dataFlow:DataFlow example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/dataflows/example
    /// ```
    /// </summary>
    [AzureResourceType("azure:datafactory/dataFlow:DataFlow")]
    public partial class DataFlow : global::Pulumi.CustomResource
    {
        /// <summary>
        /// List of tags that can be used for describing the Data Factory Data Flow.
        /// </summary>
        [Output("annotations")]
        public Output<ImmutableArray<string>> Annotations { get; private set; } = null!;

        /// <summary>
        /// The ID of Data Factory in which to associate the Data Flow with. Changing this forces a new resource.
        /// </summary>
        [Output("dataFactoryId")]
        public Output<string> DataFactoryId { get; private set; } = null!;

        /// <summary>
        /// The description for the Data Factory Data Flow.
        /// </summary>
        [Output("description")]
        public Output<string?> Description { get; private set; } = null!;

        /// <summary>
        /// The folder that this Data Flow is in. If not specified, the Data Flow will appear at the root level.
        /// </summary>
        [Output("folder")]
        public Output<string?> Folder { get; private set; } = null!;

        /// <summary>
        /// Specifies the name of the Data Factory Data Flow. Changing this forces a new resource to be created.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// The script for the Data Factory Data Flow.
        /// </summary>
        [Output("script")]
        public Output<string?> Script { get; private set; } = null!;

        /// <summary>
        /// The script lines for the Data Factory Data Flow.
        /// </summary>
        [Output("scriptLines")]
        public Output<ImmutableArray<string>> ScriptLines { get; private set; } = null!;

        /// <summary>
        /// One or more `Sink` blocks as defined below.
        /// </summary>
        [Output("sinks")]
        public Output<ImmutableArray<Outputs.DataFlowSink>> Sinks { get; private set; } = null!;

        /// <summary>
        /// One or more `Source` blocks as defined below.
        /// </summary>
        [Output("sources")]
        public Output<ImmutableArray<Outputs.DataFlowSource>> Sources { get; private set; } = null!;

        /// <summary>
        /// One or more `Transformation` blocks as defined below.
        /// </summary>
        [Output("transformations")]
        public Output<ImmutableArray<Outputs.DataFlowTransformation>> Transformations { get; private set; } = null!;


        /// <summary>
        /// Create a DataFlow resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public DataFlow(string name, DataFlowArgs args, CustomResourceOptions? options = null)
            : base("azure:datafactory/dataFlow:DataFlow", name, args ?? new DataFlowArgs(), MakeResourceOptions(options, ""))
        {
        }

        private DataFlow(string name, Input<string> id, DataFlowState? state = null, CustomResourceOptions? options = null)
            : base("azure:datafactory/dataFlow:DataFlow", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing DataFlow resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static DataFlow Get(string name, Input<string> id, DataFlowState? state = null, CustomResourceOptions? options = null)
        {
            return new DataFlow(name, id, state, options);
        }
    }

    public sealed class DataFlowArgs : global::Pulumi.ResourceArgs
    {
        [Input("annotations")]
        private InputList<string>? _annotations;

        /// <summary>
        /// List of tags that can be used for describing the Data Factory Data Flow.
        /// </summary>
        public InputList<string> Annotations
        {
            get => _annotations ?? (_annotations = new InputList<string>());
            set => _annotations = value;
        }

        /// <summary>
        /// The ID of Data Factory in which to associate the Data Flow with. Changing this forces a new resource.
        /// </summary>
        [Input("dataFactoryId", required: true)]
        public Input<string> DataFactoryId { get; set; } = null!;

        /// <summary>
        /// The description for the Data Factory Data Flow.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// The folder that this Data Flow is in. If not specified, the Data Flow will appear at the root level.
        /// </summary>
        [Input("folder")]
        public Input<string>? Folder { get; set; }

        /// <summary>
        /// Specifies the name of the Data Factory Data Flow. Changing this forces a new resource to be created.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// The script for the Data Factory Data Flow.
        /// </summary>
        [Input("script")]
        public Input<string>? Script { get; set; }

        [Input("scriptLines")]
        private InputList<string>? _scriptLines;

        /// <summary>
        /// The script lines for the Data Factory Data Flow.
        /// </summary>
        public InputList<string> ScriptLines
        {
            get => _scriptLines ?? (_scriptLines = new InputList<string>());
            set => _scriptLines = value;
        }

        [Input("sinks", required: true)]
        private InputList<Inputs.DataFlowSinkArgs>? _sinks;

        /// <summary>
        /// One or more `Sink` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowSinkArgs> Sinks
        {
            get => _sinks ?? (_sinks = new InputList<Inputs.DataFlowSinkArgs>());
            set => _sinks = value;
        }

        [Input("sources", required: true)]
        private InputList<Inputs.DataFlowSourceArgs>? _sources;

        /// <summary>
        /// One or more `Source` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowSourceArgs> Sources
        {
            get => _sources ?? (_sources = new InputList<Inputs.DataFlowSourceArgs>());
            set => _sources = value;
        }

        [Input("transformations")]
        private InputList<Inputs.DataFlowTransformationArgs>? _transformations;

        /// <summary>
        /// One or more `Transformation` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowTransformationArgs> Transformations
        {
            get => _transformations ?? (_transformations = new InputList<Inputs.DataFlowTransformationArgs>());
            set => _transformations = value;
        }

        public DataFlowArgs()
        {
        }
        public static new DataFlowArgs Empty => new DataFlowArgs();
    }

    public sealed class DataFlowState : global::Pulumi.ResourceArgs
    {
        [Input("annotations")]
        private InputList<string>? _annotations;

        /// <summary>
        /// List of tags that can be used for describing the Data Factory Data Flow.
        /// </summary>
        public InputList<string> Annotations
        {
            get => _annotations ?? (_annotations = new InputList<string>());
            set => _annotations = value;
        }

        /// <summary>
        /// The ID of Data Factory in which to associate the Data Flow with. Changing this forces a new resource.
        /// </summary>
        [Input("dataFactoryId")]
        public Input<string>? DataFactoryId { get; set; }

        /// <summary>
        /// The description for the Data Factory Data Flow.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// The folder that this Data Flow is in. If not specified, the Data Flow will appear at the root level.
        /// </summary>
        [Input("folder")]
        public Input<string>? Folder { get; set; }

        /// <summary>
        /// Specifies the name of the Data Factory Data Flow. Changing this forces a new resource to be created.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// The script for the Data Factory Data Flow.
        /// </summary>
        [Input("script")]
        public Input<string>? Script { get; set; }

        [Input("scriptLines")]
        private InputList<string>? _scriptLines;

        /// <summary>
        /// The script lines for the Data Factory Data Flow.
        /// </summary>
        public InputList<string> ScriptLines
        {
            get => _scriptLines ?? (_scriptLines = new InputList<string>());
            set => _scriptLines = value;
        }

        [Input("sinks")]
        private InputList<Inputs.DataFlowSinkGetArgs>? _sinks;

        /// <summary>
        /// One or more `Sink` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowSinkGetArgs> Sinks
        {
            get => _sinks ?? (_sinks = new InputList<Inputs.DataFlowSinkGetArgs>());
            set => _sinks = value;
        }

        [Input("sources")]
        private InputList<Inputs.DataFlowSourceGetArgs>? _sources;

        /// <summary>
        /// One or more `Source` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowSourceGetArgs> Sources
        {
            get => _sources ?? (_sources = new InputList<Inputs.DataFlowSourceGetArgs>());
            set => _sources = value;
        }

        [Input("transformations")]
        private InputList<Inputs.DataFlowTransformationGetArgs>? _transformations;

        /// <summary>
        /// One or more `Transformation` blocks as defined below.
        /// </summary>
        public InputList<Inputs.DataFlowTransformationGetArgs> Transformations
        {
            get => _transformations ?? (_transformations = new InputList<Inputs.DataFlowTransformationGetArgs>());
            set => _transformations = value;
        }

        public DataFlowState()
        {
        }
        public static new DataFlowState Empty => new DataFlowState();
    }
}
