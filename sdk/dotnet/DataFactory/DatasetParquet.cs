// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Azure.DataFactory
{
    /// <summary>
    /// Manages an Azure Parquet Dataset inside an Azure Data Factory.
    /// 
    /// ## Import
    /// 
    /// Data Factory Datasets can be imported using the `resource id`, e.g.
    /// 
    /// ```sh
    ///  $ pulumi import azure:datafactory/datasetParquet:DatasetParquet example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/datasets/example
    /// ```
    /// </summary>
    [AzureResourceType("azure:datafactory/datasetParquet:DatasetParquet")]
    public partial class DatasetParquet : global::Pulumi.CustomResource
    {
        /// <summary>
        /// A map of additional properties to associate with the Data Factory Dataset.
        /// 
        /// The following supported locations for a Parquet Dataset:
        /// </summary>
        [Output("additionalProperties")]
        public Output<ImmutableDictionary<string, string>?> AdditionalProperties { get; private set; } = null!;

        /// <summary>
        /// List of tags that can be used for describing the Data Factory Dataset.
        /// </summary>
        [Output("annotations")]
        public Output<ImmutableArray<string>> Annotations { get; private set; } = null!;

        /// <summary>
        /// A `azure_blob_fs_location` block as defined below.
        /// </summary>
        [Output("azureBlobFsLocation")]
        public Output<Outputs.DatasetParquetAzureBlobFsLocation?> AzureBlobFsLocation { get; private set; } = null!;

        /// <summary>
        /// A `azure_blob_storage_location` block as defined below.
        /// 
        /// The following supported arguments are specific to Parquet Dataset:
        /// </summary>
        [Output("azureBlobStorageLocation")]
        public Output<Outputs.DatasetParquetAzureBlobStorageLocation?> AzureBlobStorageLocation { get; private set; } = null!;

        /// <summary>
        /// The compression codec used to read/write text files. Valid values are `bzip2`, `gzip`, `deflate`, `ZipDeflate`, `TarGzip`, `Tar`, `snappy`, or `lz4`. Please note these values are case-sensitive.
        /// </summary>
        [Output("compressionCodec")]
        public Output<string?> CompressionCodec { get; private set; } = null!;

        /// <summary>
        /// Specifies the compression level. Possible values are `Optimal` and `Fastest`,
        /// </summary>
        [Output("compressionLevel")]
        public Output<string?> CompressionLevel { get; private set; } = null!;

        /// <summary>
        /// The Data Factory ID in which to associate the Dataset with. Changing this forces a new resource.
        /// </summary>
        [Output("dataFactoryId")]
        public Output<string> DataFactoryId { get; private set; } = null!;

        /// <summary>
        /// The description for the Data Factory Dataset.
        /// </summary>
        [Output("description")]
        public Output<string?> Description { get; private set; } = null!;

        /// <summary>
        /// The folder that this Dataset is in. If not specified, the Dataset will appear at the root level.
        /// </summary>
        [Output("folder")]
        public Output<string?> Folder { get; private set; } = null!;

        /// <summary>
        /// A `http_server_location` block as defined below.
        /// </summary>
        [Output("httpServerLocation")]
        public Output<Outputs.DatasetParquetHttpServerLocation?> HttpServerLocation { get; private set; } = null!;

        /// <summary>
        /// The Data Factory Linked Service name in which to associate the Dataset with.
        /// </summary>
        [Output("linkedServiceName")]
        public Output<string> LinkedServiceName { get; private set; } = null!;

        /// <summary>
        /// Specifies the name of the Data Factory Dataset. Changing this forces a new resource to be created. Must be globally unique. See the [Microsoft documentation](https://docs.microsoft.com/azure/data-factory/naming-rules) for all restrictions.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// A map of parameters to associate with the Data Factory Dataset.
        /// </summary>
        [Output("parameters")]
        public Output<ImmutableDictionary<string, string>?> Parameters { get; private set; } = null!;

        /// <summary>
        /// A `schema_column` block as defined below.
        /// </summary>
        [Output("schemaColumns")]
        public Output<ImmutableArray<Outputs.DatasetParquetSchemaColumn>> SchemaColumns { get; private set; } = null!;


        /// <summary>
        /// Create a DatasetParquet resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public DatasetParquet(string name, DatasetParquetArgs args, CustomResourceOptions? options = null)
            : base("azure:datafactory/datasetParquet:DatasetParquet", name, args ?? new DatasetParquetArgs(), MakeResourceOptions(options, ""))
        {
        }

        private DatasetParquet(string name, Input<string> id, DatasetParquetState? state = null, CustomResourceOptions? options = null)
            : base("azure:datafactory/datasetParquet:DatasetParquet", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing DatasetParquet resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static DatasetParquet Get(string name, Input<string> id, DatasetParquetState? state = null, CustomResourceOptions? options = null)
        {
            return new DatasetParquet(name, id, state, options);
        }
    }

    public sealed class DatasetParquetArgs : global::Pulumi.ResourceArgs
    {
        [Input("additionalProperties")]
        private InputMap<string>? _additionalProperties;

        /// <summary>
        /// A map of additional properties to associate with the Data Factory Dataset.
        /// 
        /// The following supported locations for a Parquet Dataset:
        /// </summary>
        public InputMap<string> AdditionalProperties
        {
            get => _additionalProperties ?? (_additionalProperties = new InputMap<string>());
            set => _additionalProperties = value;
        }

        [Input("annotations")]
        private InputList<string>? _annotations;

        /// <summary>
        /// List of tags that can be used for describing the Data Factory Dataset.
        /// </summary>
        public InputList<string> Annotations
        {
            get => _annotations ?? (_annotations = new InputList<string>());
            set => _annotations = value;
        }

        /// <summary>
        /// A `azure_blob_fs_location` block as defined below.
        /// </summary>
        [Input("azureBlobFsLocation")]
        public Input<Inputs.DatasetParquetAzureBlobFsLocationArgs>? AzureBlobFsLocation { get; set; }

        /// <summary>
        /// A `azure_blob_storage_location` block as defined below.
        /// 
        /// The following supported arguments are specific to Parquet Dataset:
        /// </summary>
        [Input("azureBlobStorageLocation")]
        public Input<Inputs.DatasetParquetAzureBlobStorageLocationArgs>? AzureBlobStorageLocation { get; set; }

        /// <summary>
        /// The compression codec used to read/write text files. Valid values are `bzip2`, `gzip`, `deflate`, `ZipDeflate`, `TarGzip`, `Tar`, `snappy`, or `lz4`. Please note these values are case-sensitive.
        /// </summary>
        [Input("compressionCodec")]
        public Input<string>? CompressionCodec { get; set; }

        /// <summary>
        /// Specifies the compression level. Possible values are `Optimal` and `Fastest`,
        /// </summary>
        [Input("compressionLevel")]
        public Input<string>? CompressionLevel { get; set; }

        /// <summary>
        /// The Data Factory ID in which to associate the Dataset with. Changing this forces a new resource.
        /// </summary>
        [Input("dataFactoryId", required: true)]
        public Input<string> DataFactoryId { get; set; } = null!;

        /// <summary>
        /// The description for the Data Factory Dataset.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// The folder that this Dataset is in. If not specified, the Dataset will appear at the root level.
        /// </summary>
        [Input("folder")]
        public Input<string>? Folder { get; set; }

        /// <summary>
        /// A `http_server_location` block as defined below.
        /// </summary>
        [Input("httpServerLocation")]
        public Input<Inputs.DatasetParquetHttpServerLocationArgs>? HttpServerLocation { get; set; }

        /// <summary>
        /// The Data Factory Linked Service name in which to associate the Dataset with.
        /// </summary>
        [Input("linkedServiceName", required: true)]
        public Input<string> LinkedServiceName { get; set; } = null!;

        /// <summary>
        /// Specifies the name of the Data Factory Dataset. Changing this forces a new resource to be created. Must be globally unique. See the [Microsoft documentation](https://docs.microsoft.com/azure/data-factory/naming-rules) for all restrictions.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("parameters")]
        private InputMap<string>? _parameters;

        /// <summary>
        /// A map of parameters to associate with the Data Factory Dataset.
        /// </summary>
        public InputMap<string> Parameters
        {
            get => _parameters ?? (_parameters = new InputMap<string>());
            set => _parameters = value;
        }

        [Input("schemaColumns")]
        private InputList<Inputs.DatasetParquetSchemaColumnArgs>? _schemaColumns;

        /// <summary>
        /// A `schema_column` block as defined below.
        /// </summary>
        public InputList<Inputs.DatasetParquetSchemaColumnArgs> SchemaColumns
        {
            get => _schemaColumns ?? (_schemaColumns = new InputList<Inputs.DatasetParquetSchemaColumnArgs>());
            set => _schemaColumns = value;
        }

        public DatasetParquetArgs()
        {
        }
        public static new DatasetParquetArgs Empty => new DatasetParquetArgs();
    }

    public sealed class DatasetParquetState : global::Pulumi.ResourceArgs
    {
        [Input("additionalProperties")]
        private InputMap<string>? _additionalProperties;

        /// <summary>
        /// A map of additional properties to associate with the Data Factory Dataset.
        /// 
        /// The following supported locations for a Parquet Dataset:
        /// </summary>
        public InputMap<string> AdditionalProperties
        {
            get => _additionalProperties ?? (_additionalProperties = new InputMap<string>());
            set => _additionalProperties = value;
        }

        [Input("annotations")]
        private InputList<string>? _annotations;

        /// <summary>
        /// List of tags that can be used for describing the Data Factory Dataset.
        /// </summary>
        public InputList<string> Annotations
        {
            get => _annotations ?? (_annotations = new InputList<string>());
            set => _annotations = value;
        }

        /// <summary>
        /// A `azure_blob_fs_location` block as defined below.
        /// </summary>
        [Input("azureBlobFsLocation")]
        public Input<Inputs.DatasetParquetAzureBlobFsLocationGetArgs>? AzureBlobFsLocation { get; set; }

        /// <summary>
        /// A `azure_blob_storage_location` block as defined below.
        /// 
        /// The following supported arguments are specific to Parquet Dataset:
        /// </summary>
        [Input("azureBlobStorageLocation")]
        public Input<Inputs.DatasetParquetAzureBlobStorageLocationGetArgs>? AzureBlobStorageLocation { get; set; }

        /// <summary>
        /// The compression codec used to read/write text files. Valid values are `bzip2`, `gzip`, `deflate`, `ZipDeflate`, `TarGzip`, `Tar`, `snappy`, or `lz4`. Please note these values are case-sensitive.
        /// </summary>
        [Input("compressionCodec")]
        public Input<string>? CompressionCodec { get; set; }

        /// <summary>
        /// Specifies the compression level. Possible values are `Optimal` and `Fastest`,
        /// </summary>
        [Input("compressionLevel")]
        public Input<string>? CompressionLevel { get; set; }

        /// <summary>
        /// The Data Factory ID in which to associate the Dataset with. Changing this forces a new resource.
        /// </summary>
        [Input("dataFactoryId")]
        public Input<string>? DataFactoryId { get; set; }

        /// <summary>
        /// The description for the Data Factory Dataset.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// The folder that this Dataset is in. If not specified, the Dataset will appear at the root level.
        /// </summary>
        [Input("folder")]
        public Input<string>? Folder { get; set; }

        /// <summary>
        /// A `http_server_location` block as defined below.
        /// </summary>
        [Input("httpServerLocation")]
        public Input<Inputs.DatasetParquetHttpServerLocationGetArgs>? HttpServerLocation { get; set; }

        /// <summary>
        /// The Data Factory Linked Service name in which to associate the Dataset with.
        /// </summary>
        [Input("linkedServiceName")]
        public Input<string>? LinkedServiceName { get; set; }

        /// <summary>
        /// Specifies the name of the Data Factory Dataset. Changing this forces a new resource to be created. Must be globally unique. See the [Microsoft documentation](https://docs.microsoft.com/azure/data-factory/naming-rules) for all restrictions.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("parameters")]
        private InputMap<string>? _parameters;

        /// <summary>
        /// A map of parameters to associate with the Data Factory Dataset.
        /// </summary>
        public InputMap<string> Parameters
        {
            get => _parameters ?? (_parameters = new InputMap<string>());
            set => _parameters = value;
        }

        [Input("schemaColumns")]
        private InputList<Inputs.DatasetParquetSchemaColumnGetArgs>? _schemaColumns;

        /// <summary>
        /// A `schema_column` block as defined below.
        /// </summary>
        public InputList<Inputs.DatasetParquetSchemaColumnGetArgs> SchemaColumns
        {
            get => _schemaColumns ?? (_schemaColumns = new InputList<Inputs.DatasetParquetSchemaColumnGetArgs>());
            set => _schemaColumns = value;
        }

        public DatasetParquetState()
        {
        }
        public static new DatasetParquetState Empty => new DatasetParquetState();
    }
}
